\newpage
\maketitle
\begin{center}
\Large \textbf{第3章 模型训练} \quad 
\end{center}
\begin{abstract}
在本章中我们将详细讲解用于金融交易的Transformer网络的模型训练和预测过程。
\end{abstract}
\section{模型训练与预测概述概述}
\subsection{训练过程}
下面我们来看模型的训练过程，训练入口程序如下所示：
\lstset{language=PYTHON, caption={模型训练入口}, label={c003-train-entry-point}, basicstyle = \ttfamily}
\begin{lstlisting}
    def train(self):
        cmd_args = self.parse_args()
        stock_symbol = 'sh600260'
        batch_size = cmd_args.batch_size
        NUM_CLS = 3
        cmd_args.embedding_size = 5
        seq_length = 11
        cmd_args.num_heads = 4
        cmd_args.depth = 6 # 原始值为2
        train_iter, test_iter = self.load_stock_dataset(stock_symbol, batch_size)
        cmd_args.num_heads = 8
        model = FmtsTransformer(emb=cmd_args.embedding_size, heads=cmd_args.num_heads, depth=cmd_args.depth, \
                    seq_length=seq_length, num_tokens=cmd_args.vocab_size, num_classes=NUM_CLS, \
                    max_pool=cmd_args.max_pool)
        model.to(self.device)
        opt = torch.optim.Adam(lr=cmd_args.lr, params=model.parameters())
        sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (cmd_args.lr_warmup / cmd_args.batch_size), 1.0))
        if cmd_args.continue_train:
            e, model_dict, optimizer_dict = self.load_ckpt(self.ckpt_file)
            model.load_state_dict(model_dict)
            opt.load_state_dict(optimizer_dict)
        # training loop
        cmd_args.num_epochs = 3
        seen = 0
        # early stopping参数
        best_acc = -1
        acc_up = 0.0
        min_acc_up = 0.000001 # 识别为精度提高的最小阈值
        non_acc_up_epochs = 0 # 目前多少个epoch精度未提高
        max_no_acc_up_epochs = 50 # 如果精度在这些epoch后还没提高则终止训练过程
        for epoch in range(cmd_args.num_epochs):
            print(f'\n epoch {epoch}')
            model.train(True)
            for batch in tqdm.tqdm(train_iter):
                opt.zero_grad()
                X, y = self.get_stock_batch_sample(batch, batch_size, cmd_args.embedding_size)
                y_hat = model(X)
                loss = F.nll_loss(y_hat, y)
                loss.backward()
                # clip gradients
                # - If the total gradient vector has a length > 1, we clip it back down to 1.
                if cmd_args.gradient_clipping > 0.0:
                    nn.utils.clip_grad_norm_(model.parameters(), cmd_args.gradient_clipping)
                opt.step()
                sch.step()
                seen += X.size(0)
            with torch.no_grad():
                model.train(False)
                tot, cor= 0.0, 0.0 
                for batch in tqdm.tqdm(test_iter):
                    X, y = self.get_stock_batch_sample(batch, batch_size, cmd_args.embedding_size)
                    y_hat = model(X).argmax(dim=1)
                    tot += float(X.size(0))
                    cor += float((y == y_hat).sum().item())
                acc = cor / tot
                # 获取当前最佳测试集精度，并保存对应的模型
                if best_acc < acc:
                    acc_up = acc - best_acc
                    if acc_up > min_acc_up:
                        best_acc = acc
                        non_acc_up_epochs = 0
                        print('保存模型参数')
                        self.save_ckpt(self.ckpt_file, epoch, model, opt)
                else:
                    non_acc_up_epochs += 1
                    if non_acc_up_epochs > max_no_acc_up_epochs:
                        print('模型已经处于饱合状态，停止训练过程')
                        break
                print(f'-- {"test" if cmd_args.final else "validation"} accuracy {acc:.3}')
\end{lstlisting}
代码解读如下所示：
\begin{itemize}
    \item 第5行：Transformer网络输出三种市场状态：上涨、下跌、震荡；
    \item 第6行：我们将每个时刻等价为一个单词，每个时刻可以用开盘、最高、最低、收盘、交易量来表示，因此相当于每个单词是5维向量；
    \item 第7行：我们通常考虑当前时刻，同时向前看10个时刻，因此每个样本包括11个时刻，相当于每个句子有11个单词，所以序列长度为11；
    \item 第8行：共有6层结构；
    \item 第10行：共有8个自注意头；
    \item 第12$\sim$15行：初始化模型，其中vocab\_size=50000为缺省值，max\_pool代表使用最大池化，并将其放入GPU中；
    \item 第16行：使用adam优化算法；
    \item 第17行：使用带有warmup的学习调整计划算法：...；
    \item 第18$\sim$21行：在之前训练的基础上继续训练；
    \item 第22行：训练集训练遍数；
    \item 第23行：共训练了多少个样本；
    \item 第26行：当前所取得的最佳精度；
    \item 第27行：当前精度提高的数值；
    \item 第28行：视为精度有提高的最小阈值，用于避免拢动情况；
    \item 第29行：目前已经有多少个epoch精度没有提高，如果达到最大允许的精度没有提高的epoch数，则停止训练过程；
    \item 第30行：允许连续多少个epoch精度没有显著提高；
    \item 第31行：训练指定个epoch；
    \item 第33行：将模型置为训练状态；
    \item 第34行：读出训练集的每个迷你批次，循环进行处理；
    \item 第35行：所有参数的梯度设置为0；
    \item 第36行：获取样本集和标签集，具体实现细节将在后面详细讲解；
    \item 第37行：调用FmtsTransformer模型计算网络输出结果；
    \item 第38行：使用负对数似然函数为代价函数，计算代价函数的值；
    \item 第39行：将代价函数值反向传播，求出梯度值；
    \item 第42、43行：当梯度所组成的向量长度大于1时，调整梯度值，使长度变为1；
    \item 第44行：调用优化器对参数进行优化；
    \item 第45行：调用学习率变化方法，按需调整学习率；
    \item 第46行：统计当前学习了多少个样本；
    \item 第47、48行：将其变为验证过程；
    \item 第49行：定义tot代表总样本数，cor为结果正确的样本数；
    \item 第50行：按批次循环处理测试数据集；
    \item 第51行：取出一个批次的样本集和标签集；
    \item 第52行：调用FmtsTransformer模型的前向传播过程，求出网络层输出logits，然后在每个样本结果向量中取出最大值的索引值（取值范围为0$\sim$2），作为预测值；
    \item 第53行：将批次数加入到样本总数中；
    \item 第54行：y=y\_hat的运算结果为一个新的同维度ndarray，对于第$i$个元素，如果$y_{i}==\hat{y}_{i}$，则该元素为1，否则为0。对新数据取sum就可以计算出在本批次中
    正确的样本数量；
    \item 第55行：当处理完所有测试集后，求出在测试集上的精度；
    \item 第57$\sim$63行：如果当前精度高于最佳精度，求出精度的提升量，如果提升量高于认为有提升的阈值，则更新最佳精度至当前精度，将连续没有精度提高的epoch数置为0，保
    存模型至文件；
    \item 第64$\sim$68行：如果没有精度提升，则连续没有精度提升的epoch数加1，如果该值大于允许连续没有精度提升epoch的最大值，则退出训练过程；
\end{itemize}

\subsection{预测过程}

\section{总结}