{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-theme",
   "metadata": {},
   "source": [
    "## 5.3 交易策略\n",
    "我们在前两节中，已经研究了最简化版本的强化学习环境MinuteBarEnv。正面我们来怎样使用DQN（Deep Q-Learning）来开发一个股票交易策略。\n",
    "\n",
    "### 5.3.1. DQN算法实现\n",
    "根据前面章节的分析，DQN由Worker NN和Target NN组成。我们先来看Worker NN的实现。\n",
    "\n",
    "#### 5.3.1.1. $\\epsilon$ Greedy策略\n",
    "我们要在$1-\\epsilon$的概率下使用策略选择的行动，在$\\epsilon$的概率下使用随机策略，如下所示：\n",
    "$$\n",
    "a = \\begin{cases} \\arg \\max_{a \\in \\mathcal{A}(s)} q_{\\pi}(s, a), \\quad p=1-\\epsilon \\\\\n",
    "random, \\quad p=\\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "代码实现如下所示：\n",
    "```python\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "    def __init__(self, epsilon=0.05, selector=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.selector = selector if selector is not None else ArgmaxActionSelector()\n",
    "        print('epsilon: {0}; selector: {1};'.format(self.epsilon, self.selector))\n",
    "\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        self.epsilon = 0.7\n",
    "        batch_size, n_actions = scores.shape\n",
    "        print('batch_size={0}; n_actions={1};'.format(batch_size, n_actions))\n",
    "        actions = self.selector(scores)\n",
    "        print('actions: {0};'.format(actions))\n",
    "        mask = np.random.random(size=batch_size) < self.epsilon\n",
    "        print('mask: {0};'.format(mask))\n",
    "        rand_actions = np.random.choice(n_actions, sum(mask))\n",
    "        print('rand_actions: {0};'.format(rand_actions))\n",
    "        print('mask type: {0}; {1};'.format(type(mask), mask[2]))\n",
    "        actions[mask] = rand_actions\n",
    "        print('final actions: {0};'.format(actions))\n",
    "        return actions\n",
    "    \n",
    "class TEpsilonGreedyActionSelector(unittest.TestCase):\n",
    "    def test_usage(self):\n",
    "        selector = EpsilonGreedyActionSelector(AppConfig.EPS_START)\n",
    "        self.assertTrue(1>0)\n",
    "        scores = np.array([\n",
    "            [0.3, 0.2, 0.5, 0.1, 0.4],\n",
    "            [0.11, 0.52, 0.33, 0.65, 0.27],\n",
    "            [0.98, 0.32, 0.99, 0.15, 0.57]\n",
    "        ])\n",
    "        action = selector(scores)\n",
    "        print('action: {0};'.format(action))    \n",
    "```\n",
    "运行结果如下所示：\n",
    "```\n",
    "ionSelector.test_usage\n",
    "test_usage (uts.biz.drlt.rll.actions.t_epsilon_greedy_action_selector.TEpsilonGreedyActionSelector) ... epsilon: 1.0; selector: <biz.drlt.rll.actions.ArgmaxActionSelector object at 0x000001CFF8481C70>;\n",
    "batch_size=3; n_actions=5;\n",
    "actions: [2 3 2];\n",
    "mask: [False  True  True];\n",
    "rand_actions: [3 0];\n",
    "mask type: <class 'numpy.ndarray'>; True;\n",
    "final actions: [2 3 0];\n",
    "action: [2 3 0];\n",
    "```\n",
    "\n",
    "#### 5.3.1.1. Worker NN\n",
    "worker NN用于生成Q函数值，同时进行学习，在更新参数$N$次之后，将参数更新至Target NN。状态行动值函数（Q函数）可以表示为：\n",
    "$$\n",
    "q_{\\pi}(s, a_{,1}) = r_{,1} + v_{\\pi}(s_{,1}')|S_{t}=s_{,1}, A_{t}=a_{,1} \\\\\n",
    "q_{\\pi}(s, a_{,2}) = r_{,2} + v_{\\pi}(s_{,2}')|S_{t}=s_{,2}, A_{t}=a_{,2} \\\\\n",
    "...... \\\\\n",
    "q_{\\pi}(s, a_{,K}) = r_{,K} + v_{\\pi}(s_{,K}')|S_{t}=s_{,K}, A_{t}=a_{,K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-extraction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
