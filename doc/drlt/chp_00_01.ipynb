{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indie-cornwall",
   "metadata": {},
   "source": [
    "# 信息熵\n",
    "在本章中，我们将以多分类问题为背景，来研究信息熵问题。以强化学习中的Actor网络为例，网络的输出为可以采取的行动，我们假设可以采取的行动有$K$个，如下所示：\n",
    "$$\n",
    "a_{t} = k, \\quad k \\in \\{ 0, 1, ..., K-1 \\}\n",
    "$$\n",
    "网络的输出是采取每个行动的概率：\n",
    "$$\n",
    "p(0), p(1), ..., p(K-1)\n",
    "$$\n",
    "其中$p(k) \\in [0, 1]$代表采取第$k+1$个行动的概率，其为一个伯努利分布，而且由于通常输出层采用Softmax激活函数，因此有：\n",
    "$$\n",
    "\\sum _{k=0}^{K-1} p(k) = 1\n",
    "$$\n",
    "\n",
    "## 二分类问题\n",
    "下面我们来研究对于特定的第$k$个事件，我们用随机变量$X$来表示，用$x$来表示其可能的取值，其可能的取值只有0为不采取这个行动，1为采取这个行动。我们假定采取这个行动的概率为$y$，则有：\n",
    "$$\n",
    "P(X=0) = 1-y \\\\\n",
    "P(X=1) = y \\\\\n",
    "P(X=x) = y^{x}(1-y)^{1-x}, \\quad x \\in \\{ 0, 1 \\}\n",
    "$$\n",
    "\n",
    "## 多分类问题\n",
    "### 信息\n",
    "在一个时间点$t$，我们有$K$个可以选择的行动，我们可以将其视为一个$K$分类问题，我们用随机变量$X$来表示，其可能取值为$x \\in \\{0, 1, 2, ..., K-1 \\}$，我们可以定义香农信息，也称为自信息，或者直接称为信息$H(X)$：\n",
    "$$\n",
    "H(X) = -\\sum _{x \\in X} P(x) \\log P(x)\n",
    "$$\n",
    "在多分类问题中，我们通常用Q来代表正确的分布，P来代表我们模型表示的分布。我们以一个5个可选行动的样本为例，我们正确的概率分布Q表示为：\n",
    "$$\n",
    "[0.0, 0.0, 1.0, 0.0, 0.0]\n",
    "$$\n",
    "代表我们应该选择$k=2$所代表的第3个行动。\n",
    "根据我们模型P预测的概率分布可能为如下所示：\n",
    "$$\n",
    "[p_{0}, p_{1}, p_{2}, p_{3}, p_{4}]=[0.02, 0.08, 0.2, 0.1, 0.6] \n",
    "$$\n",
    "我们看到模型的概率分布$P$和正确的概率分布$Q$是不一样的，我们学习的任务就是要使$P$尽量与$Q$一致。为此我们需要引入交叉熵的概念。\n",
    "### 交叉熵\n",
    "交叉熵的定义为：\n",
    "$$\n",
    "H(P, Q) = -\\sum _{x \\in X} P(x) \\log Q(x)\n",
    "$$\n",
    "以上节的例子为例，其计算公式为：\n",
    "$$\n",
    "H(P, Q) = -\\sum _{x \\in X} P(x) \\log Q(x) \\\\\n",
    "= -0.0 \\times \\log 0.02 - 0.0 \\times \\log 0.08 - 1.0 \\times \\log 0.2 - 0.0 \\times \\log 0.1 - 0.0 \\times \\log0.6 \\\\\n",
    "= \\log0.2\n",
    "$$\n",
    "从上式可以看出，我们的模型$P$在正确类别下可以取得的概率越大，交叉熵的值就越小，因此我们的学习目标就可以变为使交叉熵达到最小。\n",
    "### KL散度\n",
    "在实际应用中，我们通常用KL散度来衡量两分概率分布相似度，其值越小越相似，KL散度定义为：\n",
    "$$\n",
    "KL(P || Q) = - \\sum _{x \\in X} P(x) \\log \\frac{Q(x)}{P(x)}\n",
    "$$\n",
    "实际上我们可以证明，交叉熵和KL散度是等价的：\n",
    "$$\n",
    "H(P, Q) = -\\sum _{x \\in X} P(x) \\log Q(x) \\\\\n",
    "= -\\sum _{x \\in X} P(x) \\log \\bigg( P(x)\\frac{Q(x)}{P(x)} \\bigg) \\\\\n",
    "= -\\sum _{x \\in X} P(x) \\bigg( \\log P(x) + \\log \\frac{Q(x)}{P(x)} \\bigg) \\\\\n",
    "= -\\sum _{x \\in X} P(x)\\log P(x)  -  \\sum _{x \\in X} P(x)\\log \\frac{Q(x)}{P(x)} \\\\\n",
    "= H(X) + KL(P || Q)\n",
    "$$\n",
    "因为$H(X)=-\\sum _{x \\in X} P(x) \\log P(x)$与我们的模型无关，因此是可以忽略的。所以从学习模型的概率分布$Q$而言，求交叉熵$H(P, Q)$和KL散度$KL(P || Q)$是等价的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-central",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
