{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-theme",
   "metadata": {},
   "source": [
    "## 5.3 交易策略\n",
    "我们在前两节中，已经研究了最简化版本的强化学习环境MinuteBarEnv。正面我们来怎样使用DQN（Deep Q-Learning）来开发一个股票交易策略。\n",
    "\n",
    "### 5.3.1. DQN算法实现\n",
    "根据前面章节的分析，DQN由Worker NN和Target NN组成。我们先来看Worker NN的实现。\n",
    "\n",
    "#### 5.3.1.1. $\\epsilon$ Greedy策略\n",
    "我们要在$1-\\epsilon$的概率下使用策略选择的行动，在$\\epsilon$的概率下使用随机策略，如下所示：\n",
    "$$\n",
    "a = \\begin{cases} \\arg \\max_{a \\in \\mathcal{A}(s)} q_{\\pi}(s, a), \\quad p=1-\\epsilon \\\\\n",
    "random, \\quad p=\\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表1\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "    def __init__(self, epsilon=0.05, selector=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.selector = selector if selector is not None else ArgmaxActionSelector()\n",
    "        print('epsilon: {0}; selector: {1};'.format(self.epsilon, self.selector))\n",
    "\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        self.epsilon = 0.7\n",
    "        batch_size, n_actions = scores.shape\n",
    "        print('batch_size={0}; n_actions={1};'.format(batch_size, n_actions))\n",
    "        actions = self.selector(scores)\n",
    "        print('actions: {0};'.format(actions))\n",
    "        mask = np.random.random(size=batch_size) < self.epsilon\n",
    "        print('mask: {0};'.format(mask))\n",
    "        rand_actions = np.random.choice(n_actions, sum(mask))\n",
    "        print('rand_actions: {0};'.format(rand_actions))\n",
    "        print('mask type: {0}; {1};'.format(type(mask), mask[2]))\n",
    "        actions[mask] = rand_actions\n",
    "        print('final actions: {0};'.format(actions))\n",
    "        return actions\n",
    "    \n",
    "class TEpsilonGreedyActionSelector(unittest.TestCase):\n",
    "    def test_usage(self):\n",
    "        selector = EpsilonGreedyActionSelector(AppConfig.EPS_START)\n",
    "        self.assertTrue(1>0)\n",
    "        scores = np.array([\n",
    "            [0.3, 0.2, 0.5, 0.1, 0.4],\n",
    "            [0.11, 0.52, 0.33, 0.65, 0.27],\n",
    "            [0.98, 0.32, 0.99, 0.15, 0.57]\n",
    "        ])\n",
    "        action = selector(scores)\n",
    "        print('action: {0};'.format(action))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-ethnic",
   "metadata": {},
   "source": [
    "运行结果如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表2\n",
    "```\n",
    "ionSelector.test_usage\n",
    "test_usage (uts.biz.drlt.rll.actions.t_epsilon_greedy_action_selector.TEpsilonGreedyActionSelector) \n",
    "... epsilon: 1.0; selector: <biz.drlt.rll.actions.ArgmaxActionSelector object at 0x000001CFF8481C70>;\n",
    "batch_size=3; n_actions=5;\n",
    "actions: [2 3 2];\n",
    "mask: [False  True  True];\n",
    "rand_actions: [3 0];\n",
    "mask type: <class 'numpy.ndarray'>; True;\n",
    "final actions: [2 3 0];\n",
    "action: [2 3 0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-master",
   "metadata": {},
   "source": [
    "在表1第9行，我们设置70%的概率使用随机选择的行动，30%的概率使用策略选择的行动。使用策略选择的策略如表2第7行所示，我们使用70%的概率，生成表2第8行的mask，表明第2、3个样本使用随机选择的行动，表2第9行，是我们随机选择的行动，第11行为我们只保留了第1个样本是由策略选择的行动，第2、3个样本是随机选择的行动，最终我们返回这个结果。\n",
    "\n",
    "#### 5.3.1.2. $\\epsilon$ 衰减策略\n",
    "刚开始训练时，策略的效果很差，同时我们对环境也一无所知，因此我们需要利用随机性选择的行动，来探索环境的特性。EpsilonTracker类就为了满足这一要求而设计的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTracker:\n",
    "    \"\"\"\n",
    "    Updates epsilon according to linear schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, selector: EpsilonGreedyActionSelector,\n",
    "                 eps_start: Union[int, float],\n",
    "                 eps_final: Union[int, float],\n",
    "                 eps_frames: int):\n",
    "        self.selector = selector\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_frames = eps_frames\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame: int):\n",
    "        eps = self.eps_start - frame / self.eps_frames\n",
    "        self.selector.epsilon = max(self.eps_final, eps)\n",
    "        \n",
    "class TEpsilonTracker(unittest.TestCase):\n",
    "    def test_exp(self):\n",
    "        selector = EpsilonGreedyActionSelector()\n",
    "        et = EpsilonTracker(selector=selector, eps_start=1.0, eps_final=0.05, eps_frames=100)\n",
    "        for i in range(100):\n",
    "            et.frame(i)\n",
    "            print('{0}: epsilon={1};'.format(i, et.selector.epsilon))\n",
    "            \n",
    "# 运行结果\n",
    "0: epsilon=1.0;\n",
    "1: epsilon=0.99;\n",
    "2: epsilon=0.98;\n",
    "3: epsilon=0.97;\n",
    "4: epsilon=0.96;\n",
    "5: epsilon=0.95;\n",
    "6: epsilon=0.94;\n",
    "7: epsilon=0.9299999999999999;\n",
    "8: epsilon=0.92;\n",
    "......\n",
    "95: epsilon=0.050000000000000044;\n",
    "96: epsilon=0.05;\n",
    "97: epsilon=0.05;\n",
    "98: epsilon=0.05;\n",
    "99: epsilon=0.05;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-michigan",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
