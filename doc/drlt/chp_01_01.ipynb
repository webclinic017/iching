{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aging-discharge",
   "metadata": {},
   "source": [
    "# 1. 强化学习概述\n",
    "典型的强化学习环境如下所示：\n",
    "![强化学习环境](./images/chp_02_01_001.png)\n",
    "由上图可以看出，强化学习系统由环境和Agent组成，如下所示：\n",
    "* 在第$t$时刻，Agent观察到环境状态$S_{t}$；\n",
    "* 通过计算，Agent选择行动$A_{t}$；\n",
    "* 由于Agent采取的行动，环境转移到$S_{t+1}$，并给Agent一个奖励信号$R_{t}$；\n",
    "\n",
    "以上过程循环往复进行，直到过程结束，如游戏结束。\n",
    "一个过程（如游戏的一局）我们称之为Trajectory，我们用$\\tau$表示：\n",
    "$$\n",
    "\\tau=\\{ (R_{0}, S_{1}, A_{1}), (R_{1}, S_{2}, A_{2}), (R_{2}, S_{3}, A_{3}), ..., (R_{T-1}, S_{T}, A_{T}) \\}\n",
    "$$\n",
    "按照惯例，我们用大写字母代表随机变量，用小写字母表示该随机变量所取的值。\n",
    "我们定义在某一时刻，可观察状态为$s$，采取的行动为$a$，假定环境具有随机性，环境转移到新状态$s'$，返回给Agnet奖励为$r$，这一情况出现的概率定义为：\n",
    "$$\n",
    "p(s',r | s, a) = P_{r}(S_{t+1}=s', R_{t+1}=r | S_{t}=s, A_{t}=a)\n",
    "$$\n",
    "因为环境具有随机性，所以既使转移到$s'$，可能得到多个奖励$r$，上面定义的概率就是获得每个$r$的概率。同理，我们可以求环境转移到$s'$的概率，其为转移到状态$s'$时获取所有$r$的概率之和，如下所示：\n",
    "$$\n",
    "p(s' | s, a) = P_{r}(S_{t+1}=s' | S_{t}=s, A_{t}=a)=\\sum_{r \\in \\mathcal{R}} p(s', r | s, a) \n",
    "$$\n",
    "由于环境的随机性，在状态$s$下，采取行动$a$，会以上面定义的概率转移到不同的$s'$，既使转移到某个$s'$，也可能获取不同的奖励$r$，我们感兴趣的是在状态$s$下采取行动$a$时，可以获得奖励的期望，如下所示：\n",
    "$$\n",
    "r(s, a) = E(R_{t+1} | S_{t}=s, A_{t}=a) = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a)\n",
    "$$\n",
    "在状态$s$下，采取行动$a$，假定环境会转移到新状态$s'$，由于环境的随机性，会获得$\\{r_{1}, r_{2}, ..., r_{I}\\}$的奖励，获取某个奖励$r_{k}$的概率为：\n",
    "$$\n",
    "p_{k}=\\frac{ p(s', r_{k} | s, a) }{ \\sum_{i=1}^{I} p(s', r_{i} | s, a) } = \\frac{ p(s', r_{k} | s, a) }{ p(s' | s, a) }\n",
    "$$\n",
    "下面我们来研究在状态$s$，采取行动$a$，假定环境会转移到新状态$s'$，这时可以获得奖励的期望值为：\n",
    "$$\n",
    "r(s, a, s') = \\sum_{r \\in \\mathcal{R}} r \\frac{ p(s', r | s, a) }{ p(s' | s, a) }\n",
    "$$\n",
    "在$t$时刻，Agent在其后可以获得的累积奖励定义为：\n",
    "$$\n",
    "G_{t} = R_{t+1} + \\gamma G_{t+1} = R_{t+1} + \\gamma ( R_{t+2} + \\gamma G_{t+2} ) = ... = \\sum_{k=0}^{\\infty} \\gamma ^{k} R_{t+k+1}, \\quad 0 \\leqslant \\gamma \\leqslant 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-target",
   "metadata": {},
   "source": [
    "在$t$时刻，环境处于状态$s$，由于环境具有随机性，因此所能获得的累积奖励$G_{t}$是一个随机变量，可以取多个值，我们定义这些值的期望为状态$s$的值函数：\n",
    "$$\n",
    "v_{\\pi}(s)=E_{\\pi}(G_{t} | S_{t}=s)=E(G_{t} | S_{t}=s)=E_{\\pi} \\Big( \\sum_{k=0}^{\\infty} \\gamma ^{k} R_{t+k+1} | S_{t}=s \\Big), \\quad 0 \\leqslant \\gamma \\leqslant 1\n",
    "$$\n",
    "在上式中，下标$\\pi$代表Agent的策略，是我们要学习的对象。$v_{\\pi}(s)$通常用于衡量状态$s$有多好，从而作为选择行动的依据。\n",
    "在实际应用中，我们同样希望知道在状态$s$下采取行动$a$后，我们可以获取到累积奖励$G_{t}$的希望值，如下所示：\n",
    "$$\n",
    "q_{\\pi}(s, a) = E_{\\pi}(G_{t} | S_{t}=s, A_{t}=a)=E_{\\pi} \\Big( \\sum_{k=0}^{\\infty} \\gamma ^{k} R_{t+k+1} | S_{t}=s, A_{t}=a \\Big), \\quad 0 \\leqslant \\gamma \\leqslant 1\n",
    "$$\n",
    "下面我们来推导Bellman方程，这是传统强化学习中最重要的公式，其他方法均是对该方法的近似。\n",
    "$$\n",
    "v_{\\pi}(s) = E_{\\pi}(G_{t} | S_{t}=s)=E_{\\pi} \\Big( \\sum_{k=0}^{\\infty} \\gamma ^{k} R_{t+k+1} | S_{t}=s \\Big) = E_{\\pi}( R_{t+1} + \\gamma G_{t+1} | S_{t}=s ) = \\sum_{\\pi} \\pi (a|s) \\sum_{s',r} p(s',r|s,a) ( r + \\gamma v_{\\pi}(s') )\n",
    "$$\n",
    "在上式中，$\\pi (a|s)$表示在状态$s$下采取行动$a$的概率，上式用期望的计算方法为值与其所对应概率乘积之和来表示。\n",
    "我们定义在状态$s$下采取行动$a$的情况取得的累积奖励，如下所示：\n",
    "$$\n",
    "q_{\\pi}(s,a) = E_{\\pi} ( G_{t} | S_{t}=s, A_{t}=a ) = E_{\\pi} ( \\sum_{k=0}^{\\infty} \\gamma ^{k} R_{t+k+1} )\n",
    "$$\n",
    "\n",
    "我们定义最佳的策略$\\pi ^{*}$为：\n",
    "$$\n",
    "\\forall s \\in \\mathcal{S}: \\quad \\quad v_{\\pi ^{*}}(s) \\geqslant v_{\\pi}(s)\n",
    "$$\n",
    "当策略为最佳策略$\\pi ^{*}$时，状态值函数$v_{*}$表示为：\n",
    "$$\n",
    "v_{*}(s)=\\max_{\\pi} v_{\\pi}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-crime",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
