{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-theme",
   "metadata": {},
   "source": [
    "## 5.3 交易策略\n",
    "我们在前两节中，已经研究了最简化版本的强化学习环境MinuteBarEnv。正面我们来怎样使用DQN（Deep Q-Learning）来开发一个股票交易策略。\n",
    "\n",
    "### 5.3.1. DQN算法实现\n",
    "根据前面章节的分析，DQN由Worker NN和Target NN组成。我们先来看Worker NN的实现。\n",
    "\n",
    "#### 5.3.1.1. $\\epsilon$ Greedy策略\n",
    "我们要在$1-\\epsilon$的概率下使用策略选择的行动，在$\\epsilon$的概率下使用随机策略，如下所示：\n",
    "$$\n",
    "a = \\begin{cases} \\arg \\max_{a \\in \\mathcal{A}(s)} q_{\\pi}(s, a), \\quad p=1-\\epsilon \\\\\n",
    "random, \\quad p=\\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表1\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "    def __init__(self, epsilon=0.05, selector=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.selector = selector if selector is not None else ArgmaxActionSelector()\n",
    "        print('epsilon: {0}; selector: {1};'.format(self.epsilon, self.selector))\n",
    "\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        self.epsilon = 0.7\n",
    "        batch_size, n_actions = scores.shape\n",
    "        print('batch_size={0}; n_actions={1};'.format(batch_size, n_actions))\n",
    "        actions = self.selector(scores)\n",
    "        print('actions: {0};'.format(actions))\n",
    "        mask = np.random.random(size=batch_size) < self.epsilon\n",
    "        print('mask: {0};'.format(mask))\n",
    "        rand_actions = np.random.choice(n_actions, sum(mask))\n",
    "        print('rand_actions: {0};'.format(rand_actions))\n",
    "        print('mask type: {0}; {1};'.format(type(mask), mask[2]))\n",
    "        actions[mask] = rand_actions\n",
    "        print('final actions: {0};'.format(actions))\n",
    "        return actions\n",
    "    \n",
    "class TEpsilonGreedyActionSelector(unittest.TestCase):\n",
    "    def test_usage(self):\n",
    "        selector = EpsilonGreedyActionSelector(AppConfig.EPS_START)\n",
    "        self.assertTrue(1>0)\n",
    "        scores = np.array([\n",
    "            [0.3, 0.2, 0.5, 0.1, 0.4],\n",
    "            [0.11, 0.52, 0.33, 0.65, 0.27],\n",
    "            [0.98, 0.32, 0.99, 0.15, 0.57]\n",
    "        ])\n",
    "        action = selector(scores)\n",
    "        print('action: {0};'.format(action))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-bernard",
   "metadata": {},
   "source": [
    "运行结果如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表2\n",
    "```\n",
    "ionSelector.test_usage\n",
    "test_usage (uts.biz.drlt.rll.actions.t_epsilon_greedy_action_selector.TEpsilonGreedyActionSelector) \n",
    "... epsilon: 1.0; selector: <biz.drlt.rll.actions.ArgmaxActionSelector object at 0x000001CFF8481C70>;\n",
    "batch_size=3; n_actions=5;\n",
    "actions: [2 3 2];\n",
    "mask: [False  True  True];\n",
    "rand_actions: [3 0];\n",
    "mask type: <class 'numpy.ndarray'>; True;\n",
    "final actions: [2 3 0];\n",
    "action: [2 3 0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-mandate",
   "metadata": {},
   "source": [
    "在表1第9行，我们设置70%的概率使用随机选择的行动，30%的概率使用策略选择的行动。使用策略选择的策略如表2第7行所示，我们使用70%的概率，生成表2第8行的mask，表明第2、3个样本使用随机选择的行动，表2第9行，是我们随机选择的行动，第11行为我们只保留了第1个样本是由策略选择的行动，第2、3个样本是随机选择的行动，最终我们返回这个结果。\n",
    "\n",
    "#### 5.3.1.2. $\\epsilon$ 衰减策略\n",
    "刚开始训练时，策略的效果很差，同时我们对环境也一无所知，因此我们需要利用随机性选择的行动，来探索环境的特性。EpsilonTracker类就为了满足这一要求而设计的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTracker:\n",
    "    \"\"\"\n",
    "    Updates epsilon according to linear schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, selector: EpsilonGreedyActionSelector,\n",
    "                 eps_start: Union[int, float],\n",
    "                 eps_final: Union[int, float],\n",
    "                 eps_frames: int):\n",
    "        self.selector = selector\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_frames = eps_frames\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame: int):\n",
    "        eps = self.eps_start - frame / self.eps_frames\n",
    "        self.selector.epsilon = max(self.eps_final, eps)\n",
    "        \n",
    "class TEpsilonTracker(unittest.TestCase):\n",
    "    def test_exp(self):\n",
    "        selector = EpsilonGreedyActionSelector()\n",
    "        et = EpsilonTracker(selector=selector, eps_start=1.0, eps_final=0.05, eps_frames=100)\n",
    "        for i in range(100):\n",
    "            et.frame(i)\n",
    "            print('{0}: epsilon={1};'.format(i, et.selector.epsilon))\n",
    "            \n",
    "# 运行结果\n",
    "0: epsilon=1.0;\n",
    "1: epsilon=0.99;\n",
    "2: epsilon=0.98;\n",
    "3: epsilon=0.97;\n",
    "4: epsilon=0.96;\n",
    "5: epsilon=0.95;\n",
    "6: epsilon=0.94;\n",
    "7: epsilon=0.9299999999999999;\n",
    "8: epsilon=0.92;\n",
    "......\n",
    "95: epsilon=0.050000000000000044;\n",
    "96: epsilon=0.05;\n",
    "97: epsilon=0.05;\n",
    "98: epsilon=0.05;\n",
    "99: epsilon=0.05;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-iraqi",
   "metadata": {},
   "source": [
    "#### 5.3.1.3. ExperienceSourceFirstLast\n",
    "该类的基类为ExperienceSource，这个类实现只保存和处理每Trajectory第一个和最后一个状态。该类的构造函数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceSource:\n",
    "    \"\"\"\n",
    "    Simple n-step experience source using single or multiple environments\n",
    "\n",
    "    Every experience contains n list of Experience entries\n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, steps_count=2, steps_delta=1, vectorized=False):\n",
    "        \"\"\"\n",
    "        Create simple experience source\n",
    "        :param env: environment or list of environments to be used\n",
    "        :param agent: callable to convert batch of states into actions to take\n",
    "        :param steps_count: count of steps to track for every experience chain\n",
    "        :param steps_delta: how many steps to do between experience items\n",
    "        :param vectorized: support of vectorized envs from OpenAI universe\n",
    "        \"\"\"\n",
    "        assert isinstance(env, (gym.Env, list, tuple))\n",
    "        assert isinstance(agent, BaseAgent)\n",
    "        assert isinstance(steps_count, int)\n",
    "        assert steps_count >= 1\n",
    "        assert isinstance(vectorized, bool)\n",
    "        if isinstance(env, (list, tuple)):\n",
    "            self.pool = env\n",
    "        else:\n",
    "            self.pool = [env]\n",
    "        self.agent = agent\n",
    "        self.steps_count = steps_count\n",
    "        self.steps_delta = steps_delta\n",
    "        self.total_rewards = []\n",
    "        self.total_steps = []\n",
    "        self.vectorized = vectorized\n",
    "        \n",
    "class ExperienceSourceFirstLast(ExperienceSource):\n",
    "    \"\"\"\n",
    "    This is a wrapper around ExperienceSource to prevent storing full trajectory in replay buffer when we need\n",
    "    only first and last states. For every trajectory piece it calculates discounted reward and emits only first\n",
    "    and last states and action taken in the first state.\n",
    "\n",
    "    If we have partial trajectory at the end of episode, last_state will be None\n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, gamma, steps_count=1, steps_delta=1, vectorized=False):\n",
    "        assert isinstance(gamma, float)\n",
    "        super(ExperienceSourceFirstLast, self).__init__(env, agent, steps_count+1, steps_delta, vectorized=vectorized)\n",
    "        self.gamma = gamma\n",
    "        self.steps = steps_count\n",
    "        \n",
    "# 调用代码\n",
    "class TExperienceSourceFirstLast(unittest.TestCase):\n",
    "    def test_exp(self):\n",
    "        \n",
    "        #\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        year = 2016\n",
    "        stock_data = BarData.load_year_data(year)\n",
    "        env = MinuteBarEnv(\n",
    "                stock_data, bars_count=AppConfig.BARS_COUNT)\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "        net = SimpleFFDQN(env.observation_space.shape[0],\n",
    "                                env.action_space.n).to(device)\n",
    "        selector = rll.actions.EpsilonGreedyActionSelector(AppConfig.EPS_START)\n",
    "        agent = DQNAgent(net, selector, device=device)\n",
    "\n",
    "        \n",
    "        exp_source = rll.experience.ExperienceSourceFirstLast(\n",
    "            env, agent, AppConfig.GAMMA, steps_count=AppConfig.REWARD_STEPS)\n",
    "        src_itr = iter(exp_source)\n",
    "        v1 = next(src_itr)\n",
    "        #print('v1: {0}; {1};'.format(type(v1), v1))\n",
    "        \n",
    "# 运行结果\n",
    "'''\n",
    "self.pool: <class 'list'>; [<TimeLimit<MinuteBarEnv<StocksEnv-v0>>>];\n",
    "self.agent: <class 'biz.drlt.rll.agent.DQNAgent'>; <biz.drlt.rll.agent.DQNAgent object at 0x00000138E3622E50>;\n",
    "self.steps_count: <class 'int'>; 3;\n",
    "self.steps_delta: <class 'int'>; 1;\n",
    "self.total_rewards: <class 'list'>; [];\n",
    "self.total_steps: <class 'list'>; [];\n",
    "self.vectorized: <class 'bool'>; False;\n",
    "self.gamma: <class 'float'>; 0.99;\n",
    "self.steps: <class 'int'>; 2;\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-model",
   "metadata": {},
   "source": [
    "我们先来看ExperienceSourceFirstLast类基类的__iter__方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b21aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        while True:\n",
    "            actions = [None] * len(states)\n",
    "            states_input = []\n",
    "            states_indices = []\n",
    "            for idx, state in enumerate(states):\n",
    "                if state is None:\n",
    "                    actions[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
    "                else:\n",
    "                    states_input.append(state)\n",
    "                    states_indices.append(idx)\n",
    "            if states_input:\n",
    "                states_actions, new_agent_states = self.agent(states_input, agent_states)\n",
    "                for idx, action in enumerate(states_actions):\n",
    "                    g_idx = states_indices[idx]\n",
    "                    actions[g_idx] = action\n",
    "                    agent_states[g_idx] = new_agent_states[idx]\n",
    "            grouped_actions = _group_list(actions, env_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b46bb3",
   "metadata": {},
   "source": [
    "代码解读如下所示：\n",
    "* 第2行：actions的值为[None]；\n",
    "* 第5行：states是以10天交易量加入持仓状态和收益率，共42个数据，所以本循环只会运行一次：\n",
    "* 第6行：state不为空，会执行第9、10行；\n",
    "* 第11行：states_input不为空，因此会执行第11$\\sim$16行；\n",
    "\n",
    "我们先来看对DQNAgent的调用，由对Agent的初始化代码agent = DQNAgent(net, selector, device=device)：\n",
    "* dqn_model为SimpleFFDQN；\n",
    "* selector为前面讲到的EpsilonGreedyActionSelector；\n",
    "* device为cuda:0；\n",
    "* preprocessor=None；\n",
    "\n",
    "我们来看一下第12行对DQNAgent的__call__方法的调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6245fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        q_v = self.dqn_model(states)\n",
    "        q = q_v.data.cpu().numpy()\n",
    "        actions = self.action_selector(q)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66e2d6",
   "metadata": {},
   "source": [
    "代码解读如下：\n",
    "* 第3、4行：由于agent_states=[None]所以不会执行；\n",
    "* 第5$\\sim$8行：由于self.preprocessor=None所以不会执行；\n",
    "* 第9行：求出$q(s,a)$的值；（未理解SimpleFFDQN中的计算方法含义？？？？？）\n",
    "* 第11行：按EpsilonGreedyActionSelector中的策略或按概率随机或取最大$q(s,a)$的行动；\n",
    "注：agent_states好像没用到。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
