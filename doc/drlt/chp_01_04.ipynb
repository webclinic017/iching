{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f138c40",
   "metadata": {},
   "source": [
    "## 1.4. DQN\n",
    "接下来我们来研究Q-Learning。我们首先来介绍两种值函数：状态值函数$v_{\\pi}(s)$和状态行动值函数$q_{\\pi}(s,a)$。在业界也将这两个函数称为Critic，即评判者，可以用于评价策略$\\pi$的好坏，并据此对策略进行改进，也就是著名的Q-Learning，以及其对应的深度学习版本DQN。\n",
    "### 1.4.1. 状态值函数\n",
    "状态值函数$v_{\\pi}(s)$表求在状态$s$可以获得的累积奖励的期望值：\n",
    "$$\n",
    "v_{\\pi}=E(G_{t}|S_{t}=s)\n",
    "$$\n",
    "#### 1.4.1.1. Monte Carlo法\n",
    "我们首先让Agent与环境互动$N$次，如玩$N$局游戏，对于每一局游戏：\n",
    "$$\n",
    "r_{1}^{1}, s_{1}^{1}, a_{1}^{1}, r_{2}^{1}, s_{2}^{1}, a_{2}^{1}, ...,r_{T_{1}}^{1},s_{T_{1}}^{1},a_{T_{1}}^{1}\n",
    "$$\n",
    "根据累称奖励定义：\n",
    "$$\n",
    "G_{t}=\\sum_{k=0}^{\\infty}\\gamma ^{k} R_{t+k+1}\n",
    "$$\n",
    "我们可以求出每个时刻累积奖励：\n",
    "$$\n",
    "G_{1}^{1},G_{2}^{1},...,G_{T_{1}}^{1}\n",
    "$$\n",
    "与每一时刻状态$s_{t}$进行组合，可以形成如下数据集：\n",
    "$$\n",
    "(s_{1}^{1},G_{1}^{1}),(s_{2}^{1},G_{2}^{1}),...,(s_{T_{1}}^{1},G_{T_{1}}^{1})\n",
    "$$\n",
    "因此所有$N$个Trajectory形成一个数据集：\n",
    "$$\n",
    "(s_{1}^{1},G_{1}^{1}),(s_{2}^{1},G_{2}^{1}),...,(s_{T_{1}}^{1},G_{T_{1}}^{1}) \\\\\n",
    "(s_{1}^{2},G_{1}^{2}),(s_{2}^{2},G_{2}^{2}),...,(s_{T_{1}}^{2},G_{T_{1}}^{2}) \\\\\n",
    "...... \\\\\n",
    "(s_{1}^{N},G_{1}^{N}),(s_{2}^{N},G_{2}^{N}),...,(s_{T_{1}}^{N},G_{T_{1}}^{N})\n",
    "$$\n",
    "对于每个样本，$s_{i}^{j}$代表在第$j$局中，第$i$个时间点的状态，为一个向量，如图像画面，$G_{i}^{j}$为其所对应的累积奖励，为一个标量数值。我们可以将其视为一个回归（Regression）任务，输入为向量$s_{i}^{j}$，输出值为$G_{i}^{j}$。\n",
    "\n",
    "#### 1.4.1.2. Temporal Difference法\n",
    "\n",
    "#### 1.4.1.3. MV vs TD\n",
    "\n",
    "### 1.4.2. 状态行动值函数\n",
    "\n",
    "### 1.4.3. Q-Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabce122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
